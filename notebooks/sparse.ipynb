{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List\n",
    "\n",
    "BPE_TOKENIZERS = [\n",
    "    \"openai-gpt\",\n",
    "    \"t5-base\",\n",
    "    \"roberta-base\",\n",
    "    \"facebook/bart-base\",\n",
    "    \"xlnet-base-cased\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"albert-base-v2\",\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"ctrl\",\n",
    "    \"google/electra-small-generator\",\n",
    "]\n",
    "\n",
    "\n",
    "class SparseVectorizer:\n",
    "    def __init__(self, model_name: str = 'bert-base-uncased', k1: float = 1.5, b: float = 0.75):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.vectorizer = self._create_vectorizer()\n",
    "        self.idf = None\n",
    "        self.doc_len = None\n",
    "        self.avgdl = None\n",
    "\n",
    "    def _create_vectorizer(self):\n",
    "        return TfidfVectorizer(\n",
    "            tokenizer=self.tokenize,\n",
    "            lowercase=True,\n",
    "            use_idf=True,\n",
    "            norm=None,\n",
    "            smooth_idf=False\n",
    "        )\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        token_ids = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "        return [self.tokenizer.decode([id]) for id in token_ids]\n",
    "\n",
    "    @property\n",
    "    def feature_names(self) -> List[str]:\n",
    "        return self.vectorizer.get_feature_names_out()\n",
    "\n",
    "    def fit(self, documents: List[str]):\n",
    "        X = self.vectorizer.fit_transform(documents)\n",
    "        self.idf = torch.FloatTensor(self.vectorizer.idf_)\n",
    "        X_torch = torch.FloatTensor(X.toarray())\n",
    "        self.doc_len = X_torch.sum(dim=1)\n",
    "        self.avgdl = self.doc_len.mean()\n",
    "        \n",
    "        # Calculate BM25 weights for the document corpus\n",
    "        len_d = self.doc_len.unsqueeze(1)\n",
    "        numerator = X_torch * (self.idf * (self.k1 + 1)).unsqueeze(0)\n",
    "        denominator = X_torch * (self.k1 * (1 - self.b + self.b * len_d / self.avgdl)) + 1.0\n",
    "        self.bm25_weights = numerator / denominator\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, documents: List[str]) -> torch.Tensor:\n",
    "        X = self.vectorizer.transform(documents)\n",
    "        X_torch = torch.FloatTensor(X.toarray())\n",
    "        \n",
    "        # Apply IDF weights only, as BM25 document-specific factors are pre-computed\n",
    "        vectors = X_torch * self.idf.unsqueeze(0)\n",
    "\n",
    "        # Apply BM25 factors\n",
    "        vectors = vectors * (self.k1 + 1) / (vectors + self.k1)\n",
    "\n",
    "        return vectors\n",
    "\n",
    "    def query(self, queries: List[str]) -> torch.Tensor:\n",
    "        query_vectors = self.transform(queries)\n",
    "        \n",
    "        # Compute similarity with pre-weighted documents\n",
    "        dot_product = torch.mm(query_vectors, self.bm25_weights.t())\n",
    "        query_norm = torch.norm(query_vectors, dim=1).unsqueeze(1)\n",
    "        doc_norm = torch.norm(self.bm25_weights, dim=1).unsqueeze(0)\n",
    "        \n",
    "        similarity = dot_product / (query_norm * doc_norm)\n",
    "        \n",
    "        # Replace NaN values with zeros\n",
    "        similarity = torch.nan_to_num(similarity, nan=0.0)\n",
    "        \n",
    "        return similarity\n",
    "\n",
    "    @classmethod\n",
    "    def from_documents(cls, documents: List[str], model_name: str = 'ctrl'):\n",
    "        vectorizer = cls(model_name=model_name)\n",
    "        vectorizer.fit(documents)\n",
    "        return vectorizer.transform(documents)\n",
    "\n",
    "    @classmethod\n",
    "    def fit_transform_query(cls, documents: List[str], queries: List[str], model_name: str = 'ctrl'):\n",
    "        vectorizer = cls(model_name=model_name)\n",
    "        vectorizer.fit(documents)\n",
    "        return vectorizer.query(queries, documents)\n",
    "\n",
    "documents = [\n",
    "    \"Artificial General Intelligence is a subfield of Machine Learning.\",\n",
    "    \"Natural Language Processing is crucial for AGI development.\",\n",
    "    \"The AGI researcher published a paper on advanced NLP techniques.\"\n",
    "]\n",
    "queries = [\"What is the relationship between AGI and NLP?\", \"Why do I care about AGI?\"]\n",
    "\n",
    "results = {}\n",
    "features = {}\n",
    "shapes = {}\n",
    "# Example usage:\n",
    "encoder = 'bert-base-uncased'\n",
    "vectorizer = SparseVectorizer(model_name=encoder)\n",
    "\n",
    "vectorizer.fit(documents)\n",
    "features[encoder] = vectorizer.feature_names\n",
    "vector_matrix = vectorizer.transform(documents)\n",
    "shapes[encoder] = vector_matrix.shape\n",
    "\n",
    "#%%\n",
    "# Scoring example\n",
    "query(vectorizer, queries)\n",
    "\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
